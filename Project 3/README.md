**1) Introduction**

Our music startup company,  Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, we are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

We'll be able to test our database and ETL pipeline by running queries and comparing our results with the expected results.



**2) Data**

We'll be working with two datasets that reside in S3. 

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song.

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.



**3) Files in repo**

This repo contains:

**_3.1) Scripts:_**

*etl.py* --> script that goes to the S3 bucket, extracts the required information from the data and inserts it into the dataframes that creates in Spark, storing them subsequently in the provided adress as parquet files.

***3.2.) Other files:***

*dl.cfg* --> configuration file with the data required to stablish a connection with AWS



**4) Instructions**

Run the *etl.py* file.

To run any of these files, type ***python3 file_path/file_name*** in the bash terminal.