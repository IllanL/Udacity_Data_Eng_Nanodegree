**1) Introduction**

The music startup business, Sparkify, has now grown their user base and song database and wants to move their processes and data onto the cloud. 

Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

We are now assigned the task of building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. We will later test the resulting database and ETL pipeline by running queries and comparing our results with the expected results.



**2) Data**

We'll be working with two datasets that reside in S3. 

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song.

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.



**3) Files in repo**

This repo contains:

**_3.1) Scripts:_**

*create_tables.py* --> creates database and tables, as described in the *sql_queries.py* script.

*etl.py* --> the nitty-gritty of the project: extracts the required information from the data and inserts it into the tables in the databases created in the script above, also as described in the *sql_queries.py* script.

*sql_queries.py* --> support script with the required SQL statements needed by the two other scripts above

*check_calculations.py* --> script that runs some calculations to check the tables created

***3.2.) Other files:***

*dwh.cfg* --> configuration file with the data required to stablish a connection with AWS



**4) Instructions**

Firstly, the *create_tables.py* file must be run, then, the *etl.py* file.

Finally, if desired, the *check_calculations.py* can be run to obtain the desired results.

To run any of these files, type ***python3 file_path/file_name*** in the bash terminal.